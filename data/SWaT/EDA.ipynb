{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cf2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from preprocessing import my_plot, analysis, generate_mean_std, SPCRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb66e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlsx2csv(filename):\n",
    "    prefix, postfix = filename.split('.')\n",
    "    df = pd.read_excel(f\"./csv/{filename}\")\n",
    "    df.columns = df.iloc[0].to_numpy()\n",
    "    df = df.iloc[1:]\n",
    "    df.to_csv(f\"./csv/{prefix}.csv\", index = False)\n",
    "    return\n",
    "\n",
    "def save_csv(filename: str, df: pd.DataFrame):\n",
    "    df.to_csv(f\"csv/{filename}\", index = False)\n",
    "    return\n",
    "\n",
    "def SWaT_generate_mean_std(df, target_features, label_features, wnd_size=5000):\n",
    "    df_fe = df[target_features].copy()\n",
    "    for target in target_features:\n",
    "        op = \"mean\"\n",
    "        df_fe[f\"{target}_{op}_{wnd_size}\"] = df_fe[target].rolling(wnd_size, min_periods = 200).mean()\n",
    "    \n",
    "    for target in target_features:\n",
    "        op = \"std\"\n",
    "        df_fe[f\"{target}_{op}_{wnd_size}\"] = df_fe[target].rolling(wnd_size, min_periods = 200).std()\n",
    "    df_fe[label_features] = df[label_features]\n",
    "    df_fe.dropna(axis = 0, inplace = True)\n",
    "    return df_fe\n",
    "\n",
    "# Generate spc labels by 6 rules\n",
    "class SPCRules(object):\n",
    "    def __init__(self, data, mean, std):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.n = len(data)\n",
    "        self.index = np.array(range(self.n))\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.upper_c = mean + std\n",
    "        self.upper_b = mean + 2 * std\n",
    "        self.upper_a = mean + 3 * std\n",
    "        self.lower_c = mean - std\n",
    "        self.lower_b = mean - 2 * std\n",
    "        self.lower_a = mean - 3 * std\n",
    "#         print(self.data.shape, self.mean.shape, self.std.shape, self.upper_c.shape, self.lower_c.shape)\n",
    "    \n",
    "    def detect(self, idx: int):\n",
    "        if idx == 1:\n",
    "            # One point beyond the 3 σ control limit\n",
    "            out = (self.data < self.lower_a) | (self.data > self.upper_a)\n",
    "        elif idx == 2:\n",
    "            # Nine or more points on one side of the centerline without crossing\n",
    "            counter = 9\n",
    "            upside = (self.data > self.mean).rolling(counter).sum()\n",
    "            downside = (self.data < self.mean).rolling(counter).sum()\n",
    "            out = (upside >= counter) | (downside >= counter)\n",
    "        elif idx == 3:\n",
    "            # Two out of three points in zone A or beyond\n",
    "            counter = 3\n",
    "            upside = (self.data > self.upper_b).rolling(counter).sum()\n",
    "            downside = (self.data < self.lower_b).rolling(counter).sum()\n",
    "            counter -= 1\n",
    "            out = (upside >= counter) | (downside >= counter)\n",
    "        elif idx == 4:\n",
    "            # Four out of five points in zone B or beyond\n",
    "            counter = 5\n",
    "            upside = (self.data > self.upper_c).rolling(counter).sum()\n",
    "            downside = (self.data < self.lower_c).rolling(counter).sum()\n",
    "            counter -= 1\n",
    "            out = (upside >= counter) | (downside >= counter)\n",
    "        elif idx == 5:\n",
    "            # Fifteen points are all in zone c\n",
    "            counter = 15\n",
    "            out = (self.data < self.upper_c) & (self.data > self.lower_c)\n",
    "            out = out.rolling(counter).sum()\n",
    "            out = (out == counter)\n",
    "        elif idx == 6:\n",
    "            # Eight continual points with none in zone c\n",
    "            counter = 8\n",
    "            out = (self.data > self.upper_c) | (self.data < self.lower_c)\n",
    "            out = out.rolling(counter).sum()\n",
    "            out = (out == counter)\n",
    "        else:\n",
    "            raise ValueError(\"Only implement rule 1~6\")\n",
    "        return out\n",
    "    \n",
    "    # Six or more points are continually increasing or decreasing\n",
    "    def rule7(self):\n",
    "        pass\n",
    "    \n",
    "    # Fourteen or more points alternate in direction\n",
    "    def rule8(self):\n",
    "        ofc8_ind = []\n",
    "        for i in range(self.n - 13):\n",
    "            d = self.data[i:i+14]\n",
    "            idx = self.index[i:i+14]\n",
    "            diff = list(v - u for u, v in zip(d, d[1:]))\n",
    "            if all(u * v < 0):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ecaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform xlsx to csv (csv is faster than xlsx)\n",
    "filename = \"SWaT_Dataset_Normal_v1.xlsx\"\n",
    "xlsx2csv(filename)\n",
    "filename = \"SWaT_Dataset_Attack_v0.xlsx\"\n",
    "xlsx2csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ee3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"SWaT_Dataset_Normal_v1.csv\"\n",
    "normal = pd.read_csv(f\"csv/{filename}\")\n",
    "\n",
    "filename = \"SWaT_Dataset_Attack_v0.csv\"\n",
    "anomaly = pd.read_csv(f\"csv/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241340bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have wrong value: \"A ttack\"\n",
    "print(np.unique(normal[\"Normal/Attack\"]), np.unique(anomaly[\"Normal/Attack\"]))\n",
    "# anomaly[\"Normal/Attack\"] = anomaly[\"Normal/Attack\"].map({'A ttack': \"Attack\", \"Attack\": \"Attack\", \"Normal\": \"Normal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normal and anomaly, because their timestamp is continual\n",
    "normal.columns = anomaly.columns # their columns have weird difference\n",
    "total = pd.concat([normal, anomaly], axis = 0)\n",
    "total\n",
    "save_csv(\"SWaT_total.csv\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ecfb1",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "- Normal(0: 496754), Anomaly(496754: 940190)\n",
    "- \"Normal\": 0, \"Attack\": 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"SWaT_total.csv\"\n",
    "total = pd.read_csv(f\"csv/{filename}\")\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d454c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Normal/Attack\"\n",
    "total[target] = total[target].map({\"Normal\": 0, \"Attack\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = [\n",
    "    'FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201',\n",
    "    'AIT202', 'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203',\n",
    "    'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301',\n",
    "    'MV302', 'MV303', 'MV304', 'P301', 'P302', 'AIT401', 'AIT402',\n",
    "    'FIT401', 'LIT401', 'P401', 'P402', 'P403', 'P404', 'UV401', 'AIT501',\n",
    "    'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504',\n",
    "    'P501', 'P502', 'PIT501', 'PIT502', 'PIT503', 'FIT601', 'P601', 'P602',\n",
    "    'P603'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736acf3d",
   "metadata": {},
   "source": [
    "## Remove dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dummy features\n",
    "op = \"std\"\n",
    "win = 5000\n",
    "lst = []\n",
    "for feature in target_features:\n",
    "    num = len(np.unique(df[f\"{feature}_{op}_{win}\"]))\n",
    "    if num < 10:\n",
    "        lst.append(feature)\n",
    "        \n",
    "print(lst)\n",
    "total.drop(columns = lst, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# 496754: 940190 是異常訊號，將前後共 500 row 加進來 (496254, 940690)\n",
    "target_points = total.iloc[496254: 940690]\n",
    "X = target_points.drop(columns = [\"Timestamp\", \"Normal/Attack\"])\n",
    "Y = target_points[\"Normal/Attack\"]\n",
    "selector = SelectKBest(f_classif, k = 30)\n",
    "X_new = selector.fit_transform(X, Y)\n",
    "lst = selector.get_feature_names_out(list(X.columns))\n",
    "print(f\"Select {len(lst)} features by p_val\")\n",
    "lst = list(lst)\n",
    "print(lst)\n",
    "lst = [\"Timestamp\"] + lst + [\"Normal/Attack\"]\n",
    "total = total[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96253f05",
   "metadata": {},
   "source": [
    "## Data synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = [\n",
    "    'FIT101', 'LIT101', 'MV101', 'P101', 'AIT203', 'FIT201',\n",
    "    'MV201', 'P203', 'P205', 'DPIT301', 'FIT301', 'LIT301', 'MV302',\n",
    "    'MV304', 'P302', 'AIT402', 'FIT401', 'LIT401', 'P402', 'UV401',\n",
    "    'AIT501', 'AIT502', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501',\n",
    "    'PIT501', 'PIT502', 'PIT503'\n",
    "]\n",
    "label_features = ['Timestamp', 'Normal/Attack']\n",
    "df = SWaT_generate_mean_std(total, target_features, label_features, wnd_size = 1000)\n",
    "df.to_csv(\"csv_fe/SWaT_mean_std.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4d9aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11010/3710321439.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fe['anomaly_label'] = df['Normal/Attack']\n",
      "/tmp/ipykernel_11010/3710321439.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fe['Timestamp'] = df['Timestamp']\n"
     ]
    }
   ],
   "source": [
    "filename = \"SWaT_mean_std.csv\"\n",
    "df = pd.read_csv(f\"csv_fe/{filename}\")\n",
    "features_num = 30\n",
    "features_name = df.columns[:features_num]\n",
    "datas = df.iloc[:, :features_num]\n",
    "datas.columns = [\"\" for _ in range(datas.shape[1])]\n",
    "means = df.iloc[:, features_num: 2 * features_num]\n",
    "means.columns = [\"\" for _ in range(means.shape[1])]\n",
    "stds = df.iloc[:, 2 * features_num: 3 * features_num]\n",
    "stds.columns = [\"\" for _ in range(stds.shape[1])]\n",
    "\n",
    "rule_gen = SPCRules(datas, means, stds)\n",
    "rule_lst = []\n",
    "for i in range(1, 7):\n",
    "    rule_df = rule_gen.detect(idx = i)\n",
    "    rule_df.columns = [f\"{name}_rule{i}\" for name in features_name]\n",
    "    rule_lst.append(rule_df)\n",
    "\n",
    "df_fe = pd.concat([df.iloc[:, :features_num]] + rule_lst, axis = 1)\n",
    "#     df_fe['fault_label'] = df['fault_label']\n",
    "df_fe['anomaly_label'] = df['Normal/Attack']\n",
    "df_fe['Timestamp'] = df['Timestamp']\n",
    "# remove dummy rule columns\n",
    "# remove_cols = ['LC51_03CV_rule2', 'LC51_03X_rule2', 'P51_06_rule2', 'T51_01_rule2', 'F51_01_rule2', 'P57_03_rule2', 'FC57_03PV_rule2', 'FC57_03CV_rule2', 'FC57_03X_rule2', 'T51_01_rule4', 'P57_03_rule4', 'FC57_03PV_rule4', 'FC57_03CV_rule4', 'FC57_03X_rule4', 'T51_01_rule5', 'T51_01_rule6', 'FC57_03PV_rule6']\n",
    "# df_fe.drop(columns = remove_cols, inplace = True)\n",
    "df_fe.to_csv(f\"csv_fe/SWaT_spc_label.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "332a992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT101\n",
      "LIT101\n",
      "MV101\n",
      "P101\n",
      "AIT203\n",
      "FIT201\n",
      "MV201\n",
      "P203\n",
      "P205\n",
      "DPIT301\n",
      "FIT301\n",
      "LIT301\n",
      "MV302\n",
      "MV304\n",
      "P302\n",
      "AIT402\n",
      "FIT401\n",
      "LIT401\n",
      "P402\n",
      "UV401\n",
      "AIT501\n",
      "AIT502\n",
      "FIT501\n",
      "FIT502\n",
      "FIT503\n",
      "FIT504\n",
      "P501\n",
      "PIT501\n",
      "PIT502\n",
      "PIT503\n",
      "FIT101_rule1\n",
      "LIT101_rule1\n",
      "MV101_rule1\n",
      "P101_rule1\n",
      "AIT203_rule1\n",
      "FIT201_rule1\n",
      "MV201_rule1\n",
      "P203_rule1\n",
      "P205_rule1\n",
      "DPIT301_rule1\n",
      "FIT301_rule1\n",
      "LIT301_rule1\n",
      "MV302_rule1\n",
      "MV304_rule1\n",
      "P302_rule1\n",
      "AIT402_rule1\n",
      "FIT401_rule1\n",
      "LIT401_rule1\n",
      "P402_rule1\n",
      "UV401_rule1\n",
      "AIT501_rule1\n",
      "AIT502_rule1\n",
      "FIT501_rule1\n",
      "FIT502_rule1\n",
      "FIT503_rule1\n",
      "FIT504_rule1\n",
      "P501_rule1\n",
      "PIT501_rule1\n",
      "PIT502_rule1\n",
      "PIT503_rule1\n",
      "FIT101_rule2\n",
      "LIT101_rule2\n",
      "MV101_rule2\n",
      "P101_rule2\n",
      "AIT203_rule2\n",
      "FIT201_rule2\n",
      "MV201_rule2\n",
      "P203_rule2\n",
      "P205_rule2\n",
      "DPIT301_rule2\n",
      "FIT301_rule2\n",
      "LIT301_rule2\n",
      "MV302_rule2\n",
      "MV304_rule2\n",
      "P302_rule2\n",
      "AIT402_rule2\n",
      "FIT401_rule2\n",
      "LIT401_rule2\n",
      "P402_rule2\n",
      "UV401_rule2\n",
      "AIT501_rule2\n",
      "AIT502_rule2\n",
      "FIT501_rule2\n",
      "FIT502_rule2\n",
      "FIT503_rule2\n",
      "FIT504_rule2\n",
      "P501_rule2\n",
      "PIT501_rule2\n",
      "PIT502_rule2\n",
      "PIT503_rule2\n",
      "FIT101_rule3\n",
      "LIT101_rule3\n",
      "MV101_rule3\n",
      "P101_rule3\n",
      "AIT203_rule3\n",
      "FIT201_rule3\n",
      "MV201_rule3\n",
      "P203_rule3\n",
      "P205_rule3\n",
      "DPIT301_rule3\n",
      "FIT301_rule3\n",
      "LIT301_rule3\n",
      "MV302_rule3\n",
      "MV304_rule3\n",
      "P302_rule3\n",
      "AIT402_rule3\n",
      "FIT401_rule3\n",
      "LIT401_rule3\n",
      "P402_rule3\n",
      "UV401_rule3\n",
      "AIT501_rule3\n",
      "AIT502_rule3\n",
      "FIT501_rule3\n",
      "FIT502_rule3\n",
      "FIT503_rule3\n",
      "FIT504_rule3\n",
      "P501_rule3\n",
      "PIT501_rule3\n",
      "PIT502_rule3\n",
      "PIT503_rule3\n",
      "FIT101_rule4\n",
      "LIT101_rule4\n",
      "MV101_rule4\n",
      "P101_rule4\n",
      "AIT203_rule4\n",
      "FIT201_rule4\n",
      "MV201_rule4\n",
      "P203_rule4\n",
      "P205_rule4\n",
      "DPIT301_rule4\n",
      "FIT301_rule4\n",
      "LIT301_rule4\n",
      "MV302_rule4\n",
      "MV304_rule4\n",
      "P302_rule4\n",
      "AIT402_rule4\n",
      "FIT401_rule4\n",
      "LIT401_rule4\n",
      "P402_rule4\n",
      "UV401_rule4\n",
      "AIT501_rule4\n",
      "AIT502_rule4\n",
      "FIT501_rule4\n",
      "FIT502_rule4\n",
      "FIT503_rule4\n",
      "FIT504_rule4\n",
      "P501_rule4\n",
      "PIT501_rule4\n",
      "PIT502_rule4\n",
      "PIT503_rule4\n",
      "FIT101_rule5\n",
      "LIT101_rule5\n",
      "MV101_rule5\n",
      "P101_rule5\n",
      "AIT203_rule5\n",
      "FIT201_rule5\n",
      "MV201_rule5\n",
      "P203_rule5\n",
      "P205_rule5\n",
      "DPIT301_rule5\n",
      "FIT301_rule5\n",
      "LIT301_rule5\n",
      "MV302_rule5\n",
      "MV304_rule5\n",
      "P302_rule5\n",
      "AIT402_rule5\n",
      "FIT401_rule5\n",
      "LIT401_rule5\n",
      "P402_rule5\n",
      "UV401_rule5\n",
      "AIT501_rule5\n",
      "AIT502_rule5\n",
      "FIT501_rule5\n",
      "FIT502_rule5\n",
      "FIT503_rule5\n",
      "FIT504_rule5\n",
      "P501_rule5\n",
      "PIT501_rule5\n",
      "PIT502_rule5\n",
      "PIT503_rule5\n",
      "FIT101_rule6\n",
      "LIT101_rule6\n",
      "MV101_rule6\n",
      "P101_rule6\n",
      "AIT203_rule6\n",
      "FIT201_rule6\n",
      "MV201_rule6\n",
      "P203_rule6\n",
      "P205_rule6\n",
      "DPIT301_rule6\n",
      "FIT301_rule6\n",
      "LIT301_rule6\n",
      "MV302_rule6\n",
      "MV304_rule6\n",
      "P302_rule6\n",
      "AIT402_rule6\n",
      "FIT401_rule6\n",
      "LIT401_rule6\n",
      "P402_rule6\n",
      "UV401_rule6\n",
      "AIT501_rule6\n",
      "AIT502_rule6\n",
      "FIT501_rule6\n",
      "FIT502_rule6\n",
      "FIT503_rule6\n",
      "FIT504_rule6\n",
      "P501_rule6\n",
      "PIT501_rule6\n",
      "PIT502_rule6\n",
      "PIT503_rule6\n",
      "anomaly_label\n",
      "Timestamp\n"
     ]
    }
   ],
   "source": [
    "for feature in df_fe.columns:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3202dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
